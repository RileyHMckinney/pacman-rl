\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Reinforcement Learning for Game Playing: Pac-Man Environment}

\author{\IEEEauthorblockN{Riley McKinney, Nathan Dow, Zain Karim}
\IEEEauthorblockA{\textit{CS 4375.001 -- Introduction to Machine Learning} \\
\textit{University of Texas at Dallas}\\
Richardson, TX, USA}
}

\maketitle

\begin{abstract}
This project proposes a reinforcement learning system for autonomous game playing using a custom Pac-Man environment. The goal is to train an agent to maximize its reward by learning optimal navigation strategies to collect pellets while avoiding an adversarial ghost. A tabular Q-Learning approach will first be implemented, followed by potential extensions into Deep Q-Networks (DQN) using PyTorch for scalable state representation. The project will evaluate agent performance based on cumulative rewards and convergence behavior across multiple training episodes.
\end{abstract}

\begin{IEEEkeywords}
Reinforcement Learning, Q-Learning, Game AI, Pac-Man, Machine Learning
\end{IEEEkeywords}

\section{Project Topic}
The proposed topic is \textbf{Reinforcement Learning for Game Playing}.  
A simplified Pac-Man grid environment will serve as the testbed for developing and analyzing reinforcement learning algorithms. The environment will simulate the classical pursuit-evasion problem, where the Pac-Man agent learns to collect pellets while avoiding a chasing ghost.

\section{Team Members}
This project will be conducted by \textbf{Riley McKinney, Nathan Dow, and Zain Karim}.

\section{Technique / Algorithm}
The planned technique is \textbf{Q-Learning}, a model-free reinforcement learning algorithm based on temporal difference updates:
\begin{equation}
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
\end{equation}
The agent will follow an $\epsilon$-greedy policy to balance exploration and exploitation.  
After verifying the basic Q-Learning setup, the project may extend to a \textbf{Deep Q-Network (DQN)} implemented with PyTorch to handle larger or more complex state spaces.

\section{Dataset / Environment Details}
This project will not rely on a static dataset. Instead, a \textbf{procedurally generated environment} will serve as the data source for learning:  
\begin{itemize}
    \item \textbf{Environment:} 10$\times$10 grid world with five randomly placed pellets.  
    \item \textbf{Agents:} Pac-Man (runner) and Ghost (seeker).  
    \item \textbf{State Space:} Encoded as discrete position pairs for both agents, yielding $10{,}000$ total states.  
    \item \textbf{Actions:} Four possible moves (up, down, left, right).  
    \item \textbf{Reward Function:} $+1$ for eating a pellet, $+20$ for clearing all pellets, $-10$ for being caught, and $-0.01$ per time step.  
\end{itemize}
Each episode will start with randomized pellet locations to encourage diverse exploration and learning stability.

\section{Coding Language / Tools}
\begin{itemize}
    \item \textbf{Language:} Python 3
    \item \textbf{Libraries:} NumPy, Matplotlib, PyGame (for visualization), and PyTorch (for future DQN work)
    \item \textbf{Planned Modules:}
    \begin{itemize}
        \item \texttt{environment.py} — Defines the Pac-Man grid and environment logic.  
        \item \texttt{agent.py} — Implements Q-Learning update and policy functions.  
        \item \texttt{main.py} — Training loop and episodic simulation.  
        \item \texttt{utils.py} — Plotting reward curves and logging results.  
    \end{itemize}
\end{itemize}

\section{Expected Results}
The project aims to demonstrate the ability of a Q-Learning agent to improve cumulative rewards over successive episodes, showing evidence of learned strategies.  
Expected outcomes include:
\begin{itemize}
    \item A working RL simulation environment with interactive visualization.  
    \item Reward convergence curves plotted across episodes.  
    \item A trained policy where Pac-Man efficiently collects pellets and avoids the ghost.  
\end{itemize}

\begin{thebibliography}{00}
\bibitem{watkins1992}
C.~J.~C.~H. Watkins and P. Dayan, ``Q-learning,'' \emph{Machine Learning}, vol.~8, pp.~279–292, 1992.
\bibitem{sutton2018}
R. S. Sutton and A. G. Barto, \emph{Reinforcement Learning: An Introduction}, 2nd ed. MIT Press, 2018.
\bibitem{ieee2025}
IEEE, ``Conference Templates,'' 2025. [Online]. Available: \url{https://www.ieee.org/conferences/publishing/templates.html}
\end{thebibliography}

\end{document}

